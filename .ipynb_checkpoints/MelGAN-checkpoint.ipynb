{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b89d955a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torchsummary'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_186937/368465305.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mweight_norm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchsummary\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msummary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torchsummary'"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from librosa.filters import mel as librosa_mel_fn\n",
    "from torch.nn.utils import weight_norm\n",
    "import numpy as np\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "983031e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find(\"Conv\") != -1:\n",
    "        m.weight.data.normal_(0.0, 0.02)\n",
    "    elif classname.find(\"BatchNorm2d\") != -1:\n",
    "        m.weight.data.normal_(1.0, 0.02)\n",
    "        m.bias.data.fill_(0)\n",
    "\n",
    "\n",
    "def WNConv1d(*args, **kwargs):\n",
    "    return weight_norm(nn.Conv1d(*args, **kwargs))\n",
    "\n",
    "\n",
    "def WNConvTranspose1d(*args, **kwargs):\n",
    "    return weight_norm(nn.ConvTranspose1d(*args, **kwargs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "300e80ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Audio2Mel(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_fft=1024,\n",
    "        hop_length=256,\n",
    "        win_length=1024,\n",
    "        sampling_rate=22050,\n",
    "        n_mel_channels=80,\n",
    "        mel_fmin=0.0,\n",
    "        mel_fmax=None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        ##############################################\n",
    "        # FFT Parameters                              #\n",
    "        ##############################################\n",
    "        window = torch.hann_window(win_length).float()\n",
    "        mel_basis = librosa_mel_fn(\n",
    "            sampling_rate, n_fft, n_mel_channels, mel_fmin, mel_fmax\n",
    "        )\n",
    "        mel_basis = torch.from_numpy(mel_basis).float()\n",
    "        self.register_buffer(\"mel_basis\", mel_basis)\n",
    "        self.register_buffer(\"window\", window)\n",
    "        self.n_fft = n_fft\n",
    "        self.hop_length = hop_length\n",
    "        self.win_length = win_length\n",
    "        self.sampling_rate = sampling_rate\n",
    "        self.n_mel_channels = n_mel_channels\n",
    "\n",
    "    def forward(self, audio):\n",
    "        p = (self.n_fft - self.hop_length) // 2\n",
    "        audio = F.pad(audio, (p, p), \"reflect\").squeeze(1)\n",
    "        fft = torch.stft(\n",
    "            audio,\n",
    "            n_fft=self.n_fft,\n",
    "            hop_length=self.hop_length,\n",
    "            win_length=self.win_length,\n",
    "            window=self.window,\n",
    "            center=False,\n",
    "        )\n",
    "        real_part, imag_part = fft.unbind(-1)\n",
    "        magnitude = torch.sqrt(real_part ** 2 + imag_part ** 2)\n",
    "        mel_output = torch.matmul(self.mel_basis, magnitude)\n",
    "        log_mel_spec = torch.log10(torch.clamp(mel_output, min=1e-5))\n",
    "        return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4b86d621",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResnetBlock(nn.Module):\n",
    "    def __init__(self, dim, dilation=1):\n",
    "        super().__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.ReflectionPad1d(dilation),\n",
    "            WNConv1d(dim, dim, kernel_size=3, dilation=dilation),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            WNConv1d(dim, dim, kernel_size=1),\n",
    "        )\n",
    "        self.shortcut = WNConv1d(dim, dim, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.shortcut(x) + self.block(x)\n",
    "    \n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, input_size, ngf, n_residual_layers):\n",
    "        super().__init__()\n",
    "        ratios = [8, 8, 2, 2]\n",
    "        self.hop_length = np.prod(ratios)\n",
    "        mult = int(2 ** len(ratios))\n",
    "\n",
    "        model = [\n",
    "            nn.ReflectionPad1d(3),\n",
    "            WNConv1d(input_size, mult * ngf, kernel_size=7, padding=0),\n",
    "        ]\n",
    "\n",
    "        # Upsample to raw audio scale\n",
    "        for i, r in enumerate(ratios):\n",
    "            model += [\n",
    "                nn.LeakyReLU(0.2),\n",
    "                WNConvTranspose1d(\n",
    "                    mult * ngf,\n",
    "                    mult * ngf // 2,\n",
    "                    kernel_size=r * 2,\n",
    "                    stride=r,\n",
    "                    padding=r // 2 + r % 2,\n",
    "                    output_padding=r % 2,\n",
    "                ),\n",
    "            ]\n",
    "\n",
    "            for j in range(n_residual_layers):\n",
    "                model += [ResnetBlock(mult * ngf // 2, dilation=3 ** j)]\n",
    "\n",
    "            mult //= 2\n",
    "\n",
    "        model += [\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.ReflectionPad1d(3),\n",
    "            WNConv1d(ngf, 1, kernel_size=7, padding=0),\n",
    "            nn.Tanh(),\n",
    "        ]\n",
    "\n",
    "        self.model = nn.Sequential(*model)\n",
    "        self.apply(weights_init)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b043b004",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NLayerDiscriminator(nn.Module):\n",
    "    def __init__(self, ndf, n_layers, downsampling_factor):\n",
    "        super().__init__()\n",
    "        model = nn.ModuleDict()\n",
    "\n",
    "        model[\"layer_0\"] = nn.Sequential(\n",
    "            nn.ReflectionPad1d(7),\n",
    "            WNConv1d(1, ndf, kernel_size=15),\n",
    "            nn.LeakyReLU(0.2, True),\n",
    "        )\n",
    "\n",
    "        nf = ndf\n",
    "        stride = downsampling_factor\n",
    "        for n in range(1, n_layers + 1):\n",
    "            nf_prev = nf\n",
    "            nf = min(nf * stride, 1024)\n",
    "\n",
    "            model[\"layer_%d\" % n] = nn.Sequential(\n",
    "                WNConv1d(\n",
    "                    nf_prev,\n",
    "                    nf,\n",
    "                    kernel_size=stride * 10 + 1,\n",
    "                    stride=stride,\n",
    "                    padding=stride * 5,\n",
    "                    groups=nf_prev // 4,\n",
    "                ),\n",
    "                nn.LeakyReLU(0.2, True),\n",
    "            )\n",
    "\n",
    "        nf = min(nf * 2, 1024)\n",
    "        model[\"layer_%d\" % (n_layers + 1)] = nn.Sequential(\n",
    "            WNConv1d(nf_prev, nf, kernel_size=5, stride=1, padding=2),\n",
    "            nn.LeakyReLU(0.2, True),\n",
    "        )\n",
    "\n",
    "        model[\"layer_%d\" % (n_layers + 2)] = WNConv1d(\n",
    "            nf, 1, kernel_size=3, stride=1, padding=1\n",
    "        )\n",
    "\n",
    "        self.model = model\n",
    "\n",
    "    def forward(self, x):\n",
    "        results = []\n",
    "        for key, layer in self.model.items():\n",
    "            x = layer(x)\n",
    "            results.append(x)\n",
    "        return results\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, num_D, ndf, n_layers, downsampling_factor):\n",
    "        super().__init__()\n",
    "        self.model = nn.ModuleDict()\n",
    "        for i in range(num_D):\n",
    "            self.model[f\"disc_{i}\"] = NLayerDiscriminator(\n",
    "                ndf, n_layers, downsampling_factor\n",
    "            )\n",
    "\n",
    "        self.downsample = nn.AvgPool1d(4, stride=2, padding=1, count_include_pad=False)\n",
    "        self.apply(weights_init)\n",
    "\n",
    "    def forward(self, x):\n",
    "        results = []\n",
    "        for key, disc in self.model.items():\n",
    "            results.append(disc(x))\n",
    "            x = self.downsample(x)\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ff957db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import yaml\n",
    "import torch\n",
    "import os\n",
    "\n",
    "\n",
    "def get_default_device():\n",
    "    if torch.cuda.is_available():\n",
    "        return \"cuda\"\n",
    "    else:\n",
    "        return \"cpu\"\n",
    "\n",
    "\n",
    "def load_model(mel2wav_path, device=get_default_device()):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        mel2wav_path (str or Path): path to the root folder of dumped text2mel\n",
    "        device (str or torch.device): device to load the model\n",
    "    \"\"\"\n",
    "    root = Path(mel2wav_path)\n",
    "    with open(root / \"args.yml\", \"r\") as f:\n",
    "        args = yaml.load(f, Loader=yaml.FullLoader)\n",
    "    netG = Generator(args.n_mel_channels, args.ngf, args.n_residual_layers).to(device)\n",
    "    netG.load_state_dict(torch.load(root / \"best_netG.pt\", map_location=device))\n",
    "    return netG\n",
    "\n",
    "\n",
    "class MelVocoder:\n",
    "    def __init__(\n",
    "        self,\n",
    "        path,\n",
    "        device=get_default_device(),\n",
    "        github=False,\n",
    "        model_name=\"multi_speaker\",\n",
    "    ):\n",
    "        self.fft = Audio2Mel().to(device)\n",
    "        if github:\n",
    "            netG = Generator(80, 32, 3).to(device)\n",
    "            root = Path(os.path.dirname(__file__)).parent\n",
    "            netG.load_state_dict(\n",
    "                torch.load(root / f\"models/{model_name}.pt\", map_location=device)\n",
    "            )\n",
    "            self.mel2wav = netG\n",
    "        else:\n",
    "            self.mel2wav = load_model(path, device)\n",
    "        self.device = device\n",
    "\n",
    "    def __call__(self, audio):\n",
    "        \"\"\"\n",
    "        Performs audio to mel conversion (See Audio2Mel in mel2wav/modules.py)\n",
    "        Args:\n",
    "            audio (torch.tensor): PyTorch tensor containing audio (batch_size, timesteps)\n",
    "        Returns:\n",
    "            torch.tensor: log-mel-spectrogram computed on input audio (batch_size, 80, timesteps)\n",
    "        \"\"\"\n",
    "        return self.fft(audio.unsqueeze(1).to(self.device))\n",
    "\n",
    "    def inverse(self, mel):\n",
    "        \"\"\"\n",
    "        Performs mel2audio conversion\n",
    "        Args:\n",
    "            mel (torch.tensor): PyTorch tensor containing log-mel spectrograms (batch_size, 80, timesteps)\n",
    "        Returns:\n",
    "            torch.tensor:  Inverted raw audio (batch_size, timesteps)\n",
    "\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            return self.mel2wav(mel.to(self.device)).squeeze(1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
