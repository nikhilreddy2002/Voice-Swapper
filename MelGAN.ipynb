{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b89d955a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from librosa.filters import mel as librosa_mel_fn\n",
    "from torch.nn.utils import weight_norm\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import yaml\n",
    "import torch\n",
    "import os\n",
    "from librosa.core import load\n",
    "from librosa.util import normalize\n",
    "import random\n",
    "#from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "300e80ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Audio2Mel(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_fft=1024,\n",
    "        hop_length=256,\n",
    "        win_length=1024,\n",
    "        sampling_rate=22050,\n",
    "        n_mel_channels=80,\n",
    "        mel_fmin=0.0,\n",
    "        mel_fmax=None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        ##############################################\n",
    "        # FFT Parameters                              #\n",
    "        ##############################################\n",
    "        window = torch.hann_window(win_length).float()\n",
    "        mel_basis = librosa_mel_fn(\n",
    "            sampling_rate, n_fft, n_mel_channels, mel_fmin, mel_fmax\n",
    "        )\n",
    "        mel_basis = torch.from_numpy(mel_basis).float()\n",
    "        self.register_buffer(\"mel_basis\", mel_basis)\n",
    "        self.register_buffer(\"window\", window)\n",
    "        self.n_fft = n_fft\n",
    "        self.hop_length = hop_length\n",
    "        self.win_length = win_length\n",
    "        self.sampling_rate = sampling_rate\n",
    "        self.n_mel_channels = n_mel_channels\n",
    "\n",
    "    def forward(self, audio):\n",
    "        p = (self.n_fft - self.hop_length) // 2\n",
    "        audio = F.pad(audio, (p, p), \"reflect\").squeeze(1)\n",
    "        fft = torch.stft(\n",
    "            audio,\n",
    "            n_fft=self.n_fft,\n",
    "            hop_length=self.hop_length,\n",
    "            win_length=self.win_length,\n",
    "            window=self.window,\n",
    "            center=False,\n",
    "        )\n",
    "        real_part, imag_part = fft.unbind(-1)\n",
    "        magnitude = torch.sqrt(real_part ** 2 + imag_part ** 2)\n",
    "        mel_output = torch.matmul(self.mel_basis, magnitude)\n",
    "        log_mel_spec = torch.log10(torch.clamp(mel_output, min=1e-5))\n",
    "        return log_mel_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4b86d621",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find(\"Conv\") != -1:\n",
    "        m.weight.data.normal_(0.0, 0.02)\n",
    "    elif classname.find(\"BatchNorm2d\") != -1:\n",
    "        m.weight.data.normal_(1.0, 0.02)\n",
    "        m.bias.data.fill_(0)\n",
    "\n",
    "def WNConv1d(*args, **kwargs):\n",
    "    return weight_norm(nn.Conv1d(*args, **kwargs))\n",
    "\n",
    "def WNConvTranspose1d(*args, **kwargs):\n",
    "    return weight_norm(nn.ConvTranspose1d(*args, **kwargs))\n",
    "\n",
    "\n",
    "class ResnetBlock(nn.Module):\n",
    "    def __init__(self, dim, dilation=1):\n",
    "        super().__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.ReflectionPad1d(dilation),\n",
    "            WNConv1d(dim, dim, kernel_size=3, dilation=dilation),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            WNConv1d(dim, dim, kernel_size=1),\n",
    "        )\n",
    "        self.shortcut = WNConv1d(dim, dim, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.shortcut(x) + self.block(x)\n",
    "    \n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, input_size, ngf, n_residual_layers):\n",
    "        super().__init__()\n",
    "        ratios = [8, 8, 2, 2]\n",
    "        self.hop_length = np.prod(ratios)\n",
    "        mult = int(2 ** len(ratios))\n",
    "\n",
    "        model = [\n",
    "            nn.ReflectionPad1d(3),\n",
    "            WNConv1d(input_size, mult * ngf, kernel_size=7, padding=0),\n",
    "        ]\n",
    "\n",
    "        # Upsample to raw audio scale\n",
    "        for i, r in enumerate(ratios):\n",
    "            model += [\n",
    "                nn.LeakyReLU(0.2),\n",
    "                WNConvTranspose1d(\n",
    "                    mult * ngf,\n",
    "                    mult * ngf // 2,\n",
    "                    kernel_size=r * 2,\n",
    "                    stride=r,\n",
    "                    padding=r // 2 + r % 2,\n",
    "                    output_padding=r % 2,\n",
    "                ),\n",
    "            ]\n",
    "\n",
    "            for j in range(n_residual_layers):\n",
    "                model += [ResnetBlock(mult * ngf // 2, dilation=3 ** j)]\n",
    "\n",
    "            mult //= 2\n",
    "\n",
    "        model += [\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.ReflectionPad1d(3),\n",
    "            WNConv1d(ngf, 1, kernel_size=7, padding=0),\n",
    "            nn.Tanh(),\n",
    "        ]\n",
    "\n",
    "        self.model = nn.Sequential(*model)\n",
    "        self.apply(weights_init)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b043b004",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NLayerDiscriminator(nn.Module):\n",
    "    def __init__(self, ndf, n_layers, downsampling_factor):\n",
    "        super().__init__()\n",
    "        model = nn.ModuleDict()\n",
    "\n",
    "        model[\"layer_0\"] = nn.Sequential(\n",
    "            nn.ReflectionPad1d(7),\n",
    "            WNConv1d(1, ndf, kernel_size=15),\n",
    "            nn.LeakyReLU(0.2, True),\n",
    "        )\n",
    "\n",
    "        nf = ndf\n",
    "        stride = downsampling_factor\n",
    "        for n in range(1, n_layers + 1):\n",
    "            nf_prev = nf\n",
    "            nf = min(nf * stride, 1024)\n",
    "\n",
    "            model[\"layer_%d\" % n] = nn.Sequential(\n",
    "                WNConv1d(\n",
    "                    nf_prev,\n",
    "                    nf,\n",
    "                    kernel_size=stride * 10 + 1,\n",
    "                    stride=stride,\n",
    "                    padding=stride * 5,\n",
    "                    groups=nf_prev // 4,\n",
    "                ),\n",
    "                nn.LeakyReLU(0.2, True),\n",
    "            )\n",
    "\n",
    "        nf = min(nf * 2, 1024)\n",
    "        model[\"layer_%d\" % (n_layers + 1)] = nn.Sequential(\n",
    "            WNConv1d(nf_prev, nf, kernel_size=5, stride=1, padding=2),\n",
    "            nn.LeakyReLU(0.2, True),\n",
    "        )\n",
    "\n",
    "        model[\"layer_%d\" % (n_layers + 2)] = WNConv1d(\n",
    "            nf, 1, kernel_size=3, stride=1, padding=1\n",
    "        )\n",
    "\n",
    "        self.model = model\n",
    "\n",
    "    def forward(self, x):\n",
    "        results = []\n",
    "        for key, layer in self.model.items():\n",
    "            x = layer(x)\n",
    "            results.append(x)\n",
    "        return results\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, num_D, ndf, n_layers, downsampling_factor):\n",
    "        super().__init__()\n",
    "        self.model = nn.ModuleDict()\n",
    "        for i in range(num_D):\n",
    "            self.model[f\"disc_{i}\"] = NLayerDiscriminator(\n",
    "                ndf, n_layers, downsampling_factor\n",
    "            )\n",
    "\n",
    "        self.downsample = nn.AvgPool1d(4, stride=2, padding=1, count_include_pad=False)\n",
    "        self.apply(weights_init)\n",
    "\n",
    "    def forward(self, x):\n",
    "        results = []\n",
    "        for key, disc in self.model.items():\n",
    "            results.append(disc(x))\n",
    "            x = self.downsample(x)\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ff957db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_default_device():\n",
    "    if torch.cuda.is_available():\n",
    "        return \"cuda\"\n",
    "    else:\n",
    "        return \"cpu\"\n",
    "\n",
    "\n",
    "def load_model(mel2wav_path, device=get_default_device()):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        mel2wav_path (str or Path): path to the root folder of dumped text2mel\n",
    "        device (str or torch.device): device to load the model\n",
    "    \"\"\"\n",
    "    root = Path(mel2wav_path)\n",
    "    with open(root / \"args.yml\", \"r\") as f:\n",
    "        args = yaml.load(f, Loader=yaml.FullLoader)\n",
    "    netG = Generator(args.n_mel_channels, args.ngf, args.n_residual_layers).to(device)\n",
    "    netG.load_state_dict(torch.load(root / \"best_netG.pt\", map_location=device))\n",
    "    return netG\n",
    "\n",
    "\n",
    "class MelVocoder:\n",
    "    def __init__(\n",
    "        self,\n",
    "        path,\n",
    "        device=get_default_device(),\n",
    "        github=False,\n",
    "        model_name=\"multi_speaker\",\n",
    "    ):\n",
    "        self.fft = Audio2Mel().to(device)\n",
    "        if github:\n",
    "            netG = Generator(80, 32, 3).to(device)\n",
    "            root = Path(os.path.dirname(__file__)).parent\n",
    "            netG.load_state_dict(\n",
    "                torch.load(root / f\"models/{model_name}.pt\", map_location=device)\n",
    "            )\n",
    "            self.mel2wav = netG\n",
    "        else:\n",
    "            self.mel2wav = load_model(path, device)\n",
    "        self.device = device\n",
    "\n",
    "    def __call__(self, audio):\n",
    "        \"\"\"\n",
    "        Performs audio to mel conversion (See Audio2Mel in mel2wav/modules.py)\n",
    "        Args:\n",
    "            audio (torch.tensor): PyTorch tensor containing audio (batch_size, timesteps)\n",
    "        Returns:\n",
    "            torch.tensor: log-mel-spectrogram computed on input audio (batch_size, 80, timesteps)\n",
    "        \"\"\"\n",
    "        return self.fft(audio.unsqueeze(1).to(self.device))\n",
    "\n",
    "    def inverse(self, mel):\n",
    "        \"\"\"\n",
    "        Performs mel2audio conversion\n",
    "        Args:\n",
    "            mel (torch.tensor): PyTorch tensor containing log-mel spectrograms (batch_size, 80, timesteps)\n",
    "        Returns:\n",
    "            torch.tensor:  Inverted raw audio (batch_size, timesteps)\n",
    "\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            return self.mel2wav(mel.to(self.device)).squeeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3c348d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def files_to_list(filename):\n",
    "    \"\"\"\n",
    "    Takes a text file of filenames and makes a list of filenames\n",
    "    \"\"\"\n",
    "    with open(filename, encoding=\"utf-8\") as f:\n",
    "        files = f.readlines()\n",
    "\n",
    "    files = [f.rstrip() for f in files]\n",
    "    return files\n",
    "\n",
    "\n",
    "class AudioDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    This is the main class that calculates the spectrogram and returns the\n",
    "    spectrogram, audio pair.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, training_files, segment_length, sampling_rate, augment=True):\n",
    "        self.sampling_rate = sampling_rate\n",
    "        self.segment_length = segment_length\n",
    "        self.audio_files = files_to_list(training_files)\n",
    "        self.audio_files = [Path(training_files).parent / x for x in self.audio_files]\n",
    "        random.seed(1234)\n",
    "        random.shuffle(self.audio_files)\n",
    "        self.augment = augment\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Read audio\n",
    "        filename = self.audio_files[index]\n",
    "        audio, sampling_rate = self.load_wav_to_torch(filename)\n",
    "        # Take segment\n",
    "        if audio.size(0) >= self.segment_length:\n",
    "            max_audio_start = audio.size(0) - self.segment_length\n",
    "            audio_start = random.randint(0, max_audio_start)\n",
    "            audio = audio[audio_start : audio_start + self.segment_length]\n",
    "        else:\n",
    "            audio = F.pad(\n",
    "                audio, (0, self.segment_length - audio.size(0)), \"constant\"\n",
    "            ).data\n",
    "\n",
    "        # audio = audio / 32768.0\n",
    "        return audio.unsqueeze(0)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.audio_files)\n",
    "\n",
    "    def load_wav_to_torch(self, full_path):\n",
    "        \"\"\"\n",
    "        Loads wavdata into torch array\n",
    "        \"\"\"\n",
    "        data, sampling_rate = load(full_path, sr=self.sampling_rate)\n",
    "        data = 0.95 * normalize(data)\n",
    "\n",
    "        if self.augment:\n",
    "            amplitude = np.random.uniform(low=0.3, high=1.0)\n",
    "            data = data * amplitude\n",
    "\n",
    "        return torch.from_numpy(data).float(), "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e9a65e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13 (main, Aug 25 2022, 23:26:10) \n[GCC 11.2.0]"
  },
  "vscode": {
   "interpreter": {
    "hash": "0a77c10386953d7aa1ee55a26f0f5ac1b4e5dd001f8ebf81b7231e3a892c2cef"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
